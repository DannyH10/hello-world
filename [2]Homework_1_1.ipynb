{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DannyH10/hello-world/blob/main/%5B2%5DHomework_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_lSsVJ9CYi3"
      },
      "source": [
        "#  CS 247 : Advanced Data Mining Learning\n",
        "## Homework 1\n",
        "\n",
        "### Due: 11:59 pm 01/16\n",
        "\n",
        "##### Please read the Homework Guidance (uploaded to Canvas) carefully and make sure you fulfill all the requirements.\n",
        "\n",
        "__Name__: Daniel Hulbert\n",
        "\n",
        "__UID__: 004056559"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kvI22PICYi5"
      },
      "source": [
        "## Problem 1: Multinomial Naive Bayes (50 pts)\n",
        "\n",
        "Consider the Multinomial Naive Bayes introduced in [lecture 02 - Probabilistic Classifiers](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/02NaiveBayes_LR.pdf) (please refer to page 10-23). In this problem, you are going to understand the derivations of Multinomial Naive Bayes and apply it on a real-world classification task.\n",
        "\n",
        "### Part 1: Parameter Derivation (10 pts)\n",
        "\n",
        "Show the derivation process to obtain the solution of the MLE estimator $\\pi$. Please use the same notations as in the lecture slides.\n",
        "\n",
        "__Hint__: The solution of $\\pi$ is given on slide P23.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfMLr4IsCYi6"
      },
      "source": [
        "#### Write Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with the quantity we are trying to maximize shown below:\n",
        "\n",
        "\\begin{align}\n",
        "        \\log L(\\theta) = \\sum_{d} (\\sum_{n} x_{dn} \\log \\beta_{y_{d}n} + \\log \\pi_{y_{d}})\n",
        "\\end{align}\n",
        "Since $\\beta_{ydn}$ and $\\pi_{yd}$ are independent of each other in this expression we can igor the terms dependont on $\\beta_{ydn}$ and focus on maximizing the pi term written below (using I for the indicator function):\n",
        "\\begin{align}\n",
        "  J(\\pi_{y_d}) = \\sum_{d} \\log \\pi_{y_{d}} = \\sum_{d} I(y_d==j) \\log \\pi_{j}\n",
        "\\end{align}\n",
        "The Lagrangian can be written as follows:\n",
        "\\begin{align}\n",
        "  La = \\sum_{d} I(y_d==j) \\log \\pi_{j} +\\lambda (\\sum_{j} \\pi_j-1)\n",
        "\\end{align}\n",
        "This can be maximized by taking the partial derivative with respect to $\\pi_j$ and setting equal to zero:\n",
        "\\begin{align}\n",
        "  \\frac{\\partial La}{\\partial\\pi_j} = \\sum_{d} \\frac{I(y_d==j)}{\\pi_{j}}  +\\lambda = 0\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "  \\pi_{j} = -\\sum_{d} \\frac{I(y_d==j)}{\\lambda}\n",
        "\\end{align}\n",
        "We can multiply both sides by $\\lambda$ and sum this expression over all of the different types of labels to solve for $\\lambda$:\n",
        "\\begin{align}\n",
        "  \\sum_{j} \\lambda \\pi_{j} = -\\sum_{j} \\sum_{d} I(y_d==j)\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "  \\lambda \\sum_{j} \\pi_{j} = -\\sum_{j} \\sum_{d} I(y_d==j)\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "  \\lambda = -\\sum_{j} \\sum_{d} I(y_d==j)\n",
        "\\end{align}\n",
        "$\\sum_{d} I(y_d==j)$ is the number of of documents with label j, which is being summed over all the different document labels (j) giving us:\n",
        "\\begin{align}\n",
        "  \\lambda = -|{D}|\n",
        "\\end{align}\n",
        "Thus the expression of $\\pi_j$ is the following:\n",
        "\\begin{align}\n",
        "  \\pi_{j} = \\sum_{d} \\frac{I(y_d==j)}{|{D}|} \n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "igXM1mpy30t5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1K6RDraCYi7"
      },
      "source": [
        "### Part 2: Text Classification (40 pts)\n",
        "\n",
        "In this part, you are going to apply the multinomial naive bayes method learned in the lecture on a real-world sentiment classification dataset. \n",
        "\n",
        "We've provided a general framework for you, please fill all the ''TODO'' slots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhoS3P8nCYi7"
      },
      "source": [
        "#### Part 2.1: Sklearn Implementation (5 pts)\n",
        "\n",
        "In this part, you are going to work on the text classification task using the multinomial naive bayes function __MultinomialNB__ implemented in the sklearn library. We've provided the data processing parts, please implememt the code for training and testing, and get the probability result using ***pred_proba*** and ***pred_log_proba***.\n",
        "\n",
        "\n",
        "__Hint__: \n",
        "\n",
        "1. You can refer to https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html to get familiar with sklearn MultinomialNB function.\n",
        "\n",
        "2. To get the predicted results, instead of directly using the ***predict*** function in MultinomialNB, please use <u>***np.argmax(..., axis=1)***</u>, and the ... part should be the probability result obtained from ***pred_proba*** or ***pred_log_proba***.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CEYbSxOdCYi7"
      },
      "outputs": [],
      "source": [
        "# load dataset, split train and test \n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian',  'comp.graphics', 'sci.med']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test  = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "arjdl6m_CYi8"
      },
      "outputs": [],
      "source": [
        "# data processing, turn the loaded data into array\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer().fit(twenty_train['data'] + twenty_test['data']) \n",
        "X_train_feature = count_vect.transform(twenty_train['data']).toarray()\n",
        "X_test_feature  = count_vect.transform(twenty_test['data']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zdqZW68NCYi8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9038a240-6c7e-4a28-b76d-31f1d268f77e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# train and test with MultinomialNB\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "    TO DO: \n",
        "    Please implement train and test using sklearn MultinomialNB.\n",
        "    You are expected to get the probability result using \"pred_proba\" and \"pred_log_proba\".\n",
        "'''\n",
        "\n",
        "# train the classifier with the training set\n",
        "classifier = MultinomialNB()\n",
        "labels = twenty_train['target']\n",
        "classifier.fit(X_train_feature, labels)\n",
        "# ps = classifier.predict_proba(X_train_feature)\n",
        "# log_ps = classifier.predict_log_proba(X_train_feature)\n",
        "\n",
        "# sklearn_p_labels = np.argmax(ps, axis=1)\n",
        "# sklearn_log_p_labels = np.argmax(log_ps, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q1REuUx5u1we"
      },
      "outputs": [],
      "source": [
        "# now run this on the test data set to see how we do\n",
        "ps = classifier.predict_proba(X_test_feature)\n",
        "log_ps = classifier.predict_log_proba(X_test_feature)\n",
        "\n",
        "# generate labels from max probability\n",
        "sklearn_p_labels = np.argmax(ps, axis=1)\n",
        "sklearn_log_p_labels = np.argmax(log_ps, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVFWlSv-CYi9"
      },
      "source": [
        "#### Part 2.2: Your Multinomial Naive Bayes Implementation (20 pts = 5 + 5 + 5 + 5)\n",
        "\n",
        "In this part, you are going to implement multinomial naive bayes function by your self, then use your own multinomial naive bayes function to finish the sentimate classification task, and get the probability result using ***predict_proba_without_log*** and ***predict_proba_with_log***.\n",
        "\n",
        "Hint: Similar to Part 1, to get the predicted results, please use <u>***np.argmax(..., axis=1)***</u>, and the ... part should be the probability result obtained from ***predict_proba_without_log*** and ***predict_proba_with_log***.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "prqy9nD8CYi9"
      },
      "outputs": [],
      "source": [
        "# My Multinomial Naive Bayes Function\n",
        "\n",
        "class My_MultinomialNB():\n",
        "    \"\"\"\n",
        "    Multinomial Naive Bayes\n",
        "    ==========  \n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float, optional (default=1.0)\n",
        "        Additive (Laplace/Lidstone) smoothing parameter\n",
        "        (0 for no smoothing).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1):\n",
        "        self.alpha = alpha\n",
        "        \n",
        "\n",
        "    def fit(self, X, y):\n",
        "#         Given feature X and label y, calculate beta and pi with a smoothing parameter alpha (laplace smoothing)\n",
        "        self.class_indicator = {}\n",
        "        for i, c in enumerate(np.unique(y)):\n",
        "            self.class_indicator[c] = i\n",
        "        self.n_class = len(self.class_indicator)\n",
        "        self.n_feats = np.shape(X)[1]\n",
        "        \n",
        "        self.beta    = np.zeros((self.n_class, self.n_feats))\n",
        "        self.pi      = np.zeros((self.n_class))\n",
        "        '''\n",
        "            TODO: \n",
        "            Please calculate self.beta and self.pi\n",
        "        '''\n",
        "        \n",
        "        # loop through each classification type\n",
        "        for classification in self.class_indicator.keys():\n",
        "            \n",
        "            # calculate beta\n",
        "            include = (y == classification)\n",
        "            c_word_total = np.sum(np.transpose(X), where=include) # distinct words in each classification type\n",
        "            word_counts = np.matmul(np.transpose(X), include) # total word count for classification\n",
        "            for w in range(self.n_feats):\n",
        "                self.beta[self.class_indicator[classification], w] = (word_counts[w] + self.alpha)/(c_word_total + self.n_feats * self.alpha)\n",
        "            \n",
        "            # calculate pi\n",
        "            self.pi[self.class_indicator[classification]] = np.sum(include)/np.shape(y)[0]\n",
        "\n",
        "    def predict_proba_without_log(self, X):\n",
        "#         Given a test dataset with feature X, calculate the predicted probability of each data point\n",
        "        prob = np.zeros((len(X), self.n_class), dtype=float)\n",
        "                       \n",
        "        '''\n",
        "            TODO: \n",
        "            Calculate probability of which class each data belongs to, using self.beta and self.pi\n",
        "        '''\n",
        "        for idx, feature in enumerate(X):\n",
        "            for c in self.class_indicator.keys():\n",
        "                jdx = self.class_indicator[c]\n",
        "                class_beta = self.beta[jdx,:]\n",
        "                prob[idx, jdx] = np.prod(class_beta ** feature) * self.pi[jdx]\n",
        "        return prob\n",
        "    \n",
        "    def predict_proba_with_log(self, X):\n",
        "        log_prob = self.predict_log_proba_with_log(X)\n",
        "        return np.exp(log_prob - np.max(log_prob, axis=1).reshape(-1, 1))\n",
        "    \n",
        "    def predict_log_proba_with_log(self, X):\n",
        "#         Given a test dataset with feature X, calculate the log probability of each data point\n",
        "        log_prob = np.zeros((len(X), self.n_class))\n",
        "        '''\n",
        "            TODO: \n",
        "            Calculate log-probability of which class each data belongs to, using self.log_beta and self.log_pi\n",
        "        '''\n",
        "        for idx, feature in enumerate(X):\n",
        "            for c in self.class_indicator.keys():\n",
        "                jdx = self.class_indicator[c]\n",
        "                class_beta = self.beta[jdx,:]\n",
        "                log_prob[idx, jdx] = np.sum(feature * np.log(class_beta)) + np.log(self.pi[jdx])\n",
        "        return log_prob   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CizEvDoVCYi-"
      },
      "outputs": [],
      "source": [
        "# train and test with My_MultinomialNB\n",
        "\n",
        "\n",
        "'''\n",
        "    TO DO: \n",
        "    Please implement train and test using My_MultinomialNB you implemented above.\n",
        "    You are expected to get the probability result using \"predict_proba_without_log\" and \"predict_proba_with_log\".\n",
        "    For this part, please use the default alpha value: alpha = 1.\n",
        "'''\n",
        "# training\n",
        "my_multi = My_MultinomialNB(alpha=1)\n",
        "labels = twenty_train['target']\n",
        "my_multi.fit(X_train_feature, labels)\n",
        "\n",
        "# run on test data set without using log probability\n",
        "prob_without_log = my_multi.predict_proba_without_log(X_test_feature)\n",
        "my_p_labels = np.argmax(prob_without_log, axis=1)\n",
        "\n",
        "# run on test data set with log probability\n",
        "prob_with_log = my_multi.predict_proba_with_log(X_test_feature)\n",
        "my_log_p_labels = np.argmax(prob_with_log, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A18vnCFjCYi_"
      },
      "source": [
        "#### Part 2.3: Complare sklearn MultinomialNB and your own My_MultinomialNB (15 pts = 2 + 2 + 2 + 2 + 2 + 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt334ZM-CYi_"
      },
      "source": [
        "In part 1 and part 2, you've calculated the probability of test data using ***pred_proba*** and ***pred_log_proba***, and ***predict_proba_without_log*** and ***predict_proba_with_log***. \n",
        "\n",
        "Please answer:\n",
        "1. Compare the accuracy, are they same or not? \n",
        "2. If they are different, please try to explain the reason.\n",
        "\n",
        "__Hint :__\n",
        "\n",
        "The accuracy of sklearn Multinomial NB with log and My_MultinomialNB with log should be larger than $0.9$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WF-4f8klCYi_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "050557db-06a7-49b0-ee8a-47c1dad78c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn MultinomialNB accuracy without log: 0.9347536617842876\n",
            "My_MultinomialNB accuracy without log: 0.36684420772303594\n",
            "sklearn MultinomialNB accuracy with log: 0.9347536617842876\n",
            "My_MultinomialNB accuracy with log: 0.9347536617842876\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    acc = np.equal(y_true, y_pred)\n",
        "    score = sum(acc)/len(acc) # calculate the percentage of the correctness\n",
        "    return score\n",
        "\n",
        "true_labels = twenty_test['target']\n",
        "\n",
        "# accuracy of sklearn MultinomialMB without log\n",
        "'''\n",
        "    TO DO: compute accuracy of sklearn MultinomialNB without log and print it out\n",
        "'''\n",
        "print('sklearn MultinomialNB accuracy without log:', accuracy(true_labels, sklearn_p_labels))\n",
        "\n",
        "\n",
        "# accuracy of My_MultinomialMB without log\n",
        "'''\n",
        "    TO DO: compute accuracy of My_MultinomialNB without log and print it out\n",
        "'''\n",
        "print('My_MultinomialNB accuracy without log:', accuracy(true_labels, my_p_labels))\n",
        "\n",
        "\n",
        "# accuracy of sklearn MultinomialMB with log\n",
        "'''\n",
        "    TO DO: compute accuracy of sklearn MultinomialNB with log and print it out\n",
        "'''\n",
        "print('sklearn MultinomialNB accuracy with log:', accuracy(true_labels, sklearn_log_p_labels))\n",
        "\n",
        "# accuracy of My_MultinomialMB with log \n",
        "'''\n",
        "    TO DO: compute accuracy of My_MultinomialNB with log and print it out\n",
        "'''\n",
        "print('My_MultinomialNB accuracy with log:', accuracy(true_labels, my_log_p_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1YD6ktsCYjA"
      },
      "source": [
        "\n",
        "#### Write Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. As shown above, the log and without log  values from the sklearn module are the same as My_MultinomialNB with log. My_MultinomialNB without log is much less accurate than the other 3.\n",
        "\n",
        "2. My_MultinomialNB without log performs significantly worse than with log because the values in the $\\beta$ matrix become too small for the memory allocated to them, due to the large number of entries in the dataset along with the scarcity of so many of the features."
      ],
      "metadata": {
        "id": "NhiCc2JJBamd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR_3cVMlACzA"
      },
      "source": [
        "## Problem 2:  Logistic Regression (50 pts)\n",
        "\n",
        "Consider the Logistic Regression introduced in [lecture 02 - Probabilistic Classifiers](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/02NaiveBayes_LR.pdf) (please refer to page 30-43). In this problem, you are going to understand the derivations of Logistic Regression and apply it on a real-world classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEf7Rc4GrIe"
      },
      "source": [
        "### Part 1: Logistic Regression Derivations (10pts)\n",
        "\n",
        "Write down the matrix form operation for gradient vector and Hessian matrix for logistic regression. You are expected to represent the 1st order and 2nd order derivatives in matrix/vector form, which means you should represent them in the form of multiplication/addition/subtraction of several vectors/matrices. Your representations shouldn’t include $\\sum_i$.\n",
        "\n",
        "\n",
        "Hint: \n",
        "1. Please refer to the derivations on slide 40-41, and turn them into matrix form operations. \n",
        "2. You can define vector p=sigmoid(dot(x, beta)) and matrix P=diag(dot(p,(1-p)))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__lVZsAGrIf"
      },
      "source": [
        "#### Write Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The first partial derivative (gradient vector) is given below:\n",
        "\\begin{align}\n",
        "  \\frac{\\partial \\log L(\\vec{\\beta})}{\\partial \\vec{\\beta_{j}}} = \\boldsymbol{X^{\\,T}}(\\vec{y}-\\vec{p}) \n",
        "\\end{align}\n",
        "where $\\vec{p}=\\sigma(\\boldsymbol{X} \\cdot \\vec{\\beta}^{\\,T})$\n",
        "\n",
        "2. The second partial derivative (Hessian Matrix):\n",
        "\\begin{align}\n",
        "  \\frac{\\partial^2 \\log L(\\vec{\\beta})}{\\partial \\vec{\\beta_{j}} \\partial \\vec{\\beta_j}^{\\,T}} = -(\\boldsymbol{X^{\\,T} P X})^{-1} \n",
        "\\end{align}\n",
        "\n",
        "where $\\boldsymbol{P}= diag(\\vec{p}\\cdot(1-\\vec{p})) $"
      ],
      "metadata": {
        "id": "YUeJiAnzCXfd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6_F5_EkGrIf"
      },
      "source": [
        "### Part 2: Sklearn Implementation (5 pts)\n",
        "\n",
        "In this part, you are going to work on the classification task on the __breast_cancer__ dataset using the logistic regression function __LogisticRegression__ implemented in the sklearn package. We've provided the data processing parts, please implememt the code for training and testing. Please use variable name **pred_y** for your predicted test results.\n",
        "\n",
        "\n",
        "__Hint__: \n",
        "\n",
        "1. You can refer to https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html to get familiar with the breast_cancer dataset.\n",
        "\n",
        "2. You can refer to https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html to get familiar with sklearn LogisticRegression function.\n",
        "\n",
        "3. You can change the **penalty** and **solver** parameters in the definition of __LogisticRegression__ to observe the effect of different penalties, and choose the one that you think is the best. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J8arc6z0GrIg"
      },
      "outputs": [],
      "source": [
        "# load dataset, split train and test \n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import preprocessing \n",
        "from numpy.linalg import inv\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "n_data, n_features = X.shape[0], X.shape[1]\n",
        "ids  = np.arange(n_data)\n",
        "np.random.seed(1)\n",
        "np.random.shuffle(ids)\n",
        "train_ids, test_ids = ids[: n_data // 2], ids[n_data // 2: ]\n",
        "\n",
        "train_X, train_y = X[train_ids], y[train_ids]\n",
        "test_X,  test_y  = X[test_ids],  y[test_ids]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bFqMeyXLGrIg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2920d9f4-a7b2-479a-a9b9-276c967ef18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96       106\n",
            "           1       0.98      0.97      0.97       179\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "# train and test with LogisticRegression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "'''\n",
        "    TO DO: \n",
        "    Please implement train and test using sklearn LogisticRegression.\n",
        "'''\n",
        "# train classifier\n",
        "classifier = LogisticRegression().fit(train_X, train_y)\n",
        "\n",
        "# test it on test data and generate probabilities\n",
        "classifier.predict(test_X)\n",
        "probs = classifier.predict_proba(test_X)\n",
        "pred_y = np.argmax(probs, axis=1)\n",
        "\n",
        "\n",
        "# print the evaluation results\n",
        "print(classification_report(pred_y, test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2dBTmcbGrIh"
      },
      "source": [
        "### Part 3: Your LogisticRegression Implementation (35pts = 5 + 10 + 10 + 5 + 5)\n",
        "\n",
        "In this part, you are going to implement logistic regression function by your self. \n",
        "\n",
        "For optimizing Logistic Regression, you should implement the matrix version (__matrix_GA_optimizer__) of Gradient Ascent algorithm, and matrix form Newton-Raphson algorithm (__matrix_Newton_optimizer__). Please use 1e-2 as your learning rate.\n",
        "\n",
        "Then, you are going to complete the __fit__ and __predict__ function in ***My_Logistic_Regreesion***.\n",
        "\n",
        "Finally, you are going to get the predicted results using your own ***My_Logistic_Regression*** function with the matrix version optimizer and the Newton-Raphson optimizer.\n",
        "\n",
        "__Hint__: \n",
        "\n",
        "There should not be any for loop inside the __matrix_GA_optimizer__ and __matrix_Newton_optimizer__. \n",
        "\n",
        "There can be for loop inside the __fit__ and __predict__ function.\n",
        "\n",
        "The accuracy of your implementation can be lower than the sklearn version but should be at least close to $0.9$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2t_w5dPsGrIh"
      },
      "outputs": [],
      "source": [
        "# Sigmoid function\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "      TO DO:\n",
        "      Please implement the sigmoid function\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "#  Two different Optimizers\n",
        "\n",
        "def matrix_GA_optimizer(W, X, y):\n",
        "#     Gradient Ascent (GA) optimizer implemented with matrix operations.\n",
        "    '''\n",
        "        TO DO: \n",
        "        Please implement the matrix version gradient ascent optimizer\n",
        "        Please use 1e-2 as your learning rate.\n",
        "    '''\n",
        "    learning_rate = 1e-2\n",
        "\n",
        "    beta = np.matmul(X,W.T)\n",
        "    sig = sigmoid(beta)\n",
        "    return W + learning_rate * np.matmul(X.T, y - sig)\n",
        "\n",
        "def matrix_Newton_optimizer(W, X, y):\n",
        "#     Newton's method optimizer implemented with matrix operations.\n",
        "    '''\n",
        "        TO DO:\n",
        "        Please implement the matrix version newton-raphson optimizer\n",
        "    '''\n",
        "    # first derivative (gradient vector)\n",
        "    beta = np.matmul(X,W)\n",
        "    prob = sigmoid(beta)\n",
        "    delta_W = np.matmul(X.T, y - prob)\n",
        "\n",
        "    # second derivative (Hessian Matrix)\n",
        "    prob = sigmoid(beta)\n",
        "    inv_prob = 1-prob\n",
        "    prob_prod = prob * inv_prob\n",
        "    identity = np.identity(np.shape(prob_prod)[0])\n",
        "    ps = identity * prob_prod\n",
        "    hess = -1 * np.matmul(X.T, np.matmul(ps, X))\n",
        "    hess_inv = np.linalg.pinv(hess)\n",
        "    \n",
        "    return W - np.matmul(hess_inv, delta_W.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l3l4l1UyGrIi"
      },
      "outputs": [],
      "source": [
        "# Your own Logistic Regression Function\n",
        "\n",
        "class My_Logistic_Regression():\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#         n_features : int, feature dimension\n",
        "#         optimizer  : function, one optimizer that takes input the model weights\n",
        "#                      and training data to perform one iteration of optimization.\n",
        "\n",
        "    # converted the intial weights from random to zero, helped converge faster\n",
        "    def __init__(self, n_features, optimizer):\n",
        "        self.W = np.zeros(n_features)#np.random.rand(n_features)\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "          TO DO:\n",
        "          Please implement the fit function for X, y using the optimizer you write\n",
        "        \"\"\"\n",
        "        # run 1000 iterations, seems to work well and is much more time efficient than larger orders of magnitude\n",
        "        reps = 1000\n",
        "        method = self.optimizer\n",
        "        for _ in range(reps):\n",
        "            self.W = method(self.W,X,y)            \n",
        "            \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "          TO DO:\n",
        "          Please complete the predict function given your learned W\n",
        "          Specifically, you need to compute the probability given X and W\n",
        "        \"\"\"\n",
        "        return np.round_(sigmoid(np.matmul(X,self.W)))\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iQKNZ3QJGrIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4f7e87b6-fbb0-4efc-da21-e6b56473cdde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 0 s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.87      0.91       121\n",
            "         1.0       0.91      0.98      0.94       164\n",
            "\n",
            "    accuracy                           0.93       285\n",
            "   macro avg       0.94      0.92      0.93       285\n",
            "weighted avg       0.93      0.93      0.93       285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 1 s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.97      0.95       104\n",
            "         1.0       0.98      0.96      0.97       181\n",
            "\n",
            "    accuracy                           0.96       285\n",
            "   macro avg       0.95      0.96      0.96       285\n",
            "weighted avg       0.96      0.96      0.96       285\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Finally, run the following code to train and test with your own My_Logistic_Regression function\n",
        "# Compare the accuracy and running time of both optimization versions (for-loop and matrix).\n",
        "for optimizer in [matrix_GA_optimizer, matrix_Newton_optimizer]:\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    my_clf = My_Logistic_Regression(n_features, optimizer)\n",
        "    my_clf.fit(train_X, train_y)\n",
        "    my_pred_y = my_clf.predict(test_X)\n",
        "    \n",
        "    \n",
        "    end_time = time.time()\n",
        "    print('Training time: %d s' % (end_time - start_time))\n",
        "    print(classification_report(my_pred_y, test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924BQQajLuka"
      },
      "source": [
        "## Bonus Problem: Poisson Regression (10 pts)\n",
        "\n",
        "Consider the Generalized Linear Model (GLM) introduced in [lecture 02 - Probabilistic Classifiers](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/02NaiveBayes_LR.pdf) (please refer to page 46-48)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN8nUxDYMXtB"
      },
      "source": [
        "### Part 1 Poisson Regression Derivation (2 pts)\n",
        "We know that the Poisson distribution is $P(y, \\lambda)=\\frac{\\lambda^ye^{-\\lambda}}{y!}$. Let $\\eta=\\ln(\\lambda)$. In Poisson regression, we use linear model to predict the value of $\\eta$, i.e., $\\eta=\\mathbf{x}^T\\boldsymbol{\\beta}$. Please write down $P(y;\\mathbf{x},\\boldsymbol{\\beta})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYhegj7ZQhIN"
      },
      "source": [
        "#### Write Your answer here:\n",
        "\n",
        "\\begin{align}\n",
        "  P(y, \\lambda)=\\frac{\\lambda^ye^{-\\lambda}}{y!} = \\frac{e^{\\ln({\\lambda^ye^{-\\lambda}})}}{y!} = \\frac{e^{\\ln({\\lambda^y})+\\ln(e^{-\\lambda})}}{y!} = \\frac{e^{y\\ln({\\lambda})-\\lambda}}{y!}\n",
        "\\end{align}\n",
        "Thus the parameters in canonical form can be defined as:\n",
        "\\begin{align}\n",
        "  a(\\eta) = λ = e^{\\eta}\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "  b(y) = \\frac{1}{y!}\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "  T(y) = y\n",
        "\\end{align}\n",
        "Substituting these into $P(y,\\lambda)$ gives us $P(y,\\eta)$:\n",
        "\\begin{align}\n",
        "  P(y,\\eta) = \\frac{e^{\\eta^{T}y-e^{\\eta}}}{y!}\n",
        "\\end{align}\n",
        "or\n",
        "\\begin{align}\n",
        "  P(y; x, \\beta) = \\frac{e^{(\\boldsymbol{x^T\\beta})^{T}y-e^{\\boldsymbol{x^T\\beta}}}}{y!}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUi2DnHRDxse"
      },
      "source": [
        "### Part 2 Optimization (1 pt)\n",
        "Given a training dataset $D=\\{\\mathbf{x}_i,y_i\\}_{i=1,\\cdots,n}$, please name an algorithm that can be used to learn the parameters $\\boldsymbol{\\beta}$. Explain your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyDZ2N1nEDTP"
      },
      "source": [
        "#### Write Your answer here:\n",
        "We could use this in the Logistical Regression algorithm if we were trying to handle a multiclass classification example. The logistical regression needs a probability model that generates outputs for discrete labels, which the Poisson distribution does. Because the Poisson distribution returns probabilities for discrete outputs, it cannot be used for classification labels on a continuous spectrum, like in a linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRgtr5zNY-1K"
      },
      "source": [
        "### Part 3 Application (2 pts)\n",
        "Please write down at least two real-world applications you think Poisson regression can solve. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq4AX9c2ZUp8"
      },
      "source": [
        "#### Write Your answer here:\n",
        "1. Number of goals scored per a game at the world cup where the features are the teams. (x is what leagues players play in, y is goals)\n",
        "2. Number of NCAA championships won by Universities. (x is funding for different athletic programs, y is championships)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeOvOAfaRhPb"
      },
      "source": [
        "### Part 4: Inference (5 pts = 2 pts + 3 pts)\n",
        "\n",
        "We want to use Poisson regression to predict the number of storms in 4 cities this winter. $\\mathbf{x}$ is the features of all the cities and $\\boldsymbol{\\beta}$ is already learned. \n",
        "* Please implement the code to predict the __expected number__ of storms for each city ($E(Y|\\mathbf{x},\\beta)$) and output your results. \n",
        "* Please implement the code to compute the probability of $P(Y\\ge5;x,\\beta)$ for each city and output your results.\n",
        "\n",
        "__Hint :__\n",
        "\n",
        "1. For Poisson distribution, $E(Y|\\lambda)=\\lambda$. \n",
        "\n",
        "2. $P(y\\ge5;x,\\beta)=1-P(y=0;x,\\beta)-P(y=1;x,\\beta)-P(y=2;x,\\beta)-P(y=3;x,\\beta)-P(y=4;x,\\beta)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1-amveJ2SXGP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "919791df-9d47-49c9-bff2-9dceda404c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected number of storms by city\n",
            "City 0 : 1.0618365465453596\n",
            "City 1 : 2.857651118063164\n",
            "City 2 : 3.158192909689767\n",
            "City 3 : 0.554327284734507\n",
            "\n",
            "\n",
            "Probability each city has more than 5 storms:\n",
            "City 0 : 0.004698623461541734\n",
            "City 1 : 0.16141172872837461\n",
            "City 2 : 0.21198156813230248\n",
            "City 3 : 0.0002756752469831392\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "X=np.array([[0.1,0.3,0.5],[2.7,1.6,0.1],[0.1,2.4,0.3],[0.6,0.7,5.0]])\n",
        "beta=np.array([0.1,0.5,-0.2])\n",
        "\n",
        "\"\"\"\n",
        " TO DO:\n",
        " Please implement the code to compute and print the P(y;x,\\beta) \n",
        "\"\"\"\n",
        "def Poisson_probability(X,beta, y):\n",
        "  eta = np.matmul(X,beta)\n",
        "  prob = np.exp(eta*y - np.exp(eta))/np.math.factorial(y)\n",
        "  return prob\n",
        "\n",
        "# Expected number of storms\n",
        "eta = np.matmul(X,beta)\n",
        "lamb = np.exp(eta)\n",
        "print('Expected number of storms by city')\n",
        "for idx, v in enumerate(lamb):\n",
        "  print('City',idx,\":\",v)\n",
        "\n",
        "# Probabilty of greater than 5 storms\n",
        "p_gt_5 = np.zeros(np.shape(X)[0])\n",
        "p_gt_5 += 1\n",
        "\n",
        "# subtract off probabilities of less than 5 from 1\n",
        "for y in range(5):\n",
        "  p_gt_5 -= Poisson_probability(X, beta, y)\n",
        "\n",
        "print('\\n\\nProbability each city has more than 5 storms:')\n",
        "for idx, v in enumerate(p_gt_5):\n",
        "  print('City',idx,\":\",v)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "[2]Homework_1-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}